{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20eb68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba7ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawings = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3b8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HandTracker():\n",
    "    \"\"\"\n",
    "    Function that returns hand tracker object\n",
    "    \"\"\"\n",
    "    mp_drawings = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hand_tracker = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "    return hand_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba09a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "hand_tracker = HandTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0de87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawLandMarks(results, image):\n",
    "    \"\"\"\n",
    "    Function that draws land marks\n",
    "    \"\"\"\n",
    "    mp_drawings = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    if results.multi_hand_landmarks:\n",
    "        for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "            mp_drawings.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "            # Joints Color\n",
    "            mp_drawings.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "            # Line Color\n",
    "            mp_drawings.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e2dacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cv2.VideoCapture(0)\n",
    "\n",
    "while stream.isOpened():\n",
    "    ret, frame = stream.read()\n",
    "    \n",
    "    image = cv2.flip(frame, 1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = hand_tracker.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = drawLandMarks(results, image)\n",
    "    cv2.imshow(\"Data Collection\", image)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "stream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7bbc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    '''\n",
    "    Extract Hand Keypoints\n",
    "    '''\n",
    "    hd_keypoints = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(21*3)\n",
    "    return hd_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82f6a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17256228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f065ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join(os.getcwd() ,'Hand Gestures') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['Up', 'Down', 'Right', 'Left', 'Stop'])\n",
    "# actions = np.array(['Stop'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc9cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Up', 'Down', 'Right', 'Left', 'Stop'], dtype='<U5')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ccf2df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.load('/home/sultan/Videos/MediaPipe/Hand Gestures/Stop/12/23.npy')\n",
    "# print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b203c1f5-4bbb-4e58-bada-681064ae3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print(len(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e0c28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8dccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da1744",
   "metadata": {},
   "source": [
    "Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "404f3954-6d86-4687-830d-9671f8c2b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 17:11:52.285126: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-25 17:11:52.286871: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "lstm = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54a1d3-e68e-4f20-811b-ab164e5b699f",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f915b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cv2.VideoCapture(0)\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        for frame_num in range(sequence_length):\n",
    "            ret, frame = stream.read()\n",
    "    \n",
    "            image = cv2.flip(frame, 1)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = hand_tracker.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image = drawLandMarks(results, image)\n",
    "            if frame_num == 0:\n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('Data Collection', image)\n",
    "                cv2.waitKey(5000)\n",
    "            else:\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('Data Collection', image)\n",
    "                \n",
    "            # NEW Export keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    stream.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79d08b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c55a3-6f74-4186-b8bd-a70e97da54d4",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb82360-d7ff-493c-b69e-e06cc92043e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/sultan/miniconda3/envs/sultan/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/sultan/miniconda3/envs/sultan/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 sklearn-0.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8485486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0206b5-218a-438a-90b0-b30b71f8c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "actions = np.array(['Up', 'Down', 'Right', 'Left', 'Stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a0c2d2-c207-422a-9c67-e0e0189a998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join(os.getcwd() ,'Hand Gestures') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['Up', 'Down', 'Right', 'Left', 'Stop'])\n",
    "# actions = np.array(['Stop'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b15ae0-c928-418a-823b-23773dd55419",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c487541f-83c1-4f5b-9b3f-5c385742a452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Up': 0, 'Down': 1, 'Right': 2, 'Left': 3, 'Stop': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa27e56-df53-4c6a-b91d-cd27d9b8454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = list(), list()\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        res_data = list()\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "            __res = np.load(res)\n",
    "            res_data.append(__res)\n",
    "        sequences.append(res_data)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef50cdd-6ac7-493d-86bb-d882785faf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b84cf5a8-85b8-48e2-95ff-05cfff53345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6952873-c304-4539-a4e1-54742d2897d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b47a79e2-234c-4881-b5c3-cee28640c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "labls = np.array(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0f8e29-0f7f-4726-8700-c707e5aab3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 30, 63)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a572c5c-4e5b-4e34-a634-67a929266f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ae65b93-8017-4834-8db7-c6a4802f82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sequences, labls, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38dacc49-a761-4dec-921b-16b049baf1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 30, 63)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2764159-994c-4910-83cf-0d634d8a5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train\", x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36fdb9eb-6790-4394-b5c4-11e0a1d4799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_test\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f330cfc3-e655-480a-9c05-3e5680fe4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y_train\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cab494f-666c-424e-bd49-0a371d68940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y_test\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bc4eb-1838-4f2f-aca5-adae88193b53",
   "metadata": {},
   "source": [
    "Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d909f4ec-1b48-4a65-9d1f-28cb49d9052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30eda2a-5996-4c66-a597-63c1945ae397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 17:12:13.998656: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Stop\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Up\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Right\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Down\n",
      "Right\n",
      "Right\n",
      "Down\n",
      "Down\n"
     ]
    }
   ],
   "source": [
    "st = cv2.VideoCapture(0)\n",
    "\n",
    "while st.isOpened():\n",
    "    ret, frame = st.read()\n",
    "    \n",
    "    image = cv2.flip(frame, 1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = hand_tracker.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = drawLandMarks(results, image)\n",
    "    \n",
    "    # 2. Prediction logic\n",
    "    keypoints = extract_keypoints(results)\n",
    "    sequence.append(keypoints)\n",
    "    sequence = sequence[-30:]\n",
    "    \n",
    "    if len(sequence) == 30:\n",
    "        res = lstm.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        print(actions[np.argmax(res)])\n",
    "    \n",
    "    # Show to screen\n",
    "    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "    # Break gracefully\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "st.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24caf1-48b4-43c9-b00e-0780ca26c41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
